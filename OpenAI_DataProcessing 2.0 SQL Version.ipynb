{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-30 16:31:37,899 - INFO - Skipping already processed custom_id: dark-78589-1-16-108\n",
      "Processing lines: 1it [00:00, 581.81it/s]\n",
      "2024-06-30 16:31:37,909 - INFO - Skipping already processed custom_id: dark-78792-1-16-152\n",
      "Processing lines: 1it [00:00, 521.42it/s]\n",
      "2024-06-30 16:31:37,916 - INFO - Skipping already processed custom_id: dark-78730-1-12-66\n",
      "Processing lines: 1it [00:00, 863.03it/s]\n",
      "2024-06-30 16:31:37,925 - INFO - Skipping already processed custom_id: dark-77577-1-16-69\n",
      "Processing lines: 1it [00:00, 484.95it/s]\n",
      "Processing files: 100%|██████████| 4/4 [00:00<00:00, 108.60it/s]\n",
      "2024-06-30 16:31:37,932 - INFO - Processing completed. Total completions: 380, Total events: 379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import RateLimitError\n",
    "import backoff\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "\n",
    "def create_db_tables(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS completions (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            custom_id TEXT UNIQUE,\n",
    "            content TEXT\n",
    "        )\n",
    "    ''')\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS checkpoints (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            file_path TEXT UNIQUE,\n",
    "            last_processed_line INTEGER\n",
    "        )\n",
    "    ''')\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS events (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            custom_id TEXT UNIQUE,\n",
    "            konsert_datum TEXT,\n",
    "            konsert_namn TEXT,\n",
    "            lokal_namn TEXT,\n",
    "            konserttyp_namn TEXT,\n",
    "            producer TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def get_checkpoint(conn, file_path):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT last_processed_line FROM checkpoints WHERE file_path = ?', (file_path,))\n",
    "    result = cursor.fetchone()\n",
    "    return result[0] if result else 0\n",
    "\n",
    "def update_checkpoint(conn, file_path, last_processed_line):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT OR REPLACE INTO checkpoints (file_path, last_processed_line)\n",
    "        VALUES (?, ?)\n",
    "    ''', (file_path, last_processed_line))\n",
    "    conn.commit()\n",
    "\n",
    "def extract_and_store_event_data(cursor, custom_id, json_response):\n",
    "    try:\n",
    "        event_data = json.loads(json_response)\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO events \n",
    "            (custom_id, konsert_datum, konsert_namn, lokal_namn, konserttyp_namn, producer)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            custom_id,\n",
    "            event_data.get('konsert_datum', ''),\n",
    "            event_data.get('konsert_namn', ''),\n",
    "            event_data.get('lokal_namn', ''),\n",
    "            event_data.get('konserttyp_namn', ''),\n",
    "            event_data.get('Producer', '')\n",
    "        ))\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"Error decoding JSON for custom_id: {custom_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error storing event data for custom_id {custom_id}: {e}\")\n",
    "\n",
    "def process_jsonl(file_path, db_conn):\n",
    "    cursor = db_conn.cursor()\n",
    "    last_processed_line = get_checkpoint(db_conn, file_path)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Skip to the last processed line\n",
    "            for _ in range(last_processed_line):\n",
    "                next(file)\n",
    "            \n",
    "            # Count remaining lines for progress bar\n",
    "            remaining_lines = sum(1 for _ in file) - last_processed_line\n",
    "            file.seek(0, 0)  # Reset file pointer\n",
    "            for _ in range(last_processed_line):\n",
    "                next(file)\n",
    "            \n",
    "            for current_line, line in tqdm(enumerate(file, start=last_processed_line), total=remaining_lines, desc=\"Processing lines\"):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    messages = data['body']['messages']\n",
    "                    custom_id = data.get('custom_id', f\"line_{current_line}\")\n",
    "                    \n",
    "                    # Check if this custom_id has already been processed\n",
    "                    cursor.execute('SELECT id FROM completions WHERE custom_id = ?', (custom_id,))\n",
    "                    if cursor.fetchone():\n",
    "                        logging.info(f\"Skipping already processed custom_id: {custom_id}\")\n",
    "                        continue\n",
    "                    \n",
    "                    completion = client.chat.completions.create(\n",
    "                        model='gpt-3.5-turbo',\n",
    "                        response_format={\"type\": \"json_object\"},\n",
    "                        messages=messages,\n",
    "                        max_tokens=data['body']['max_tokens']\n",
    "                    )\n",
    "                    \n",
    "                    json_response = completion.choices[0].message.content\n",
    "                    \n",
    "                    # Store the result in the completions table\n",
    "                    cursor.execute('INSERT INTO completions (custom_id, content) VALUES (?, ?)',\n",
    "                                   (custom_id, json_response))\n",
    "                    \n",
    "                    # Extract and store event data\n",
    "                    extract_and_store_event_data(cursor, custom_id, json_response)\n",
    "                    \n",
    "                    # Update checkpoint every 10 lines\n",
    "                    if current_line % 10 == 0:\n",
    "                        update_checkpoint(db_conn, file_path, current_line)\n",
    "                        db_conn.commit()\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(f\"Error decoding JSON at line {current_line}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error during API call at line {current_line}: {e}\")\n",
    "                \n",
    "            # Final checkpoint update\n",
    "            update_checkpoint(db_conn, file_path, current_line)\n",
    "            db_conn.commit()\n",
    "            \n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error opening or reading the file: {file_path}. Error: {e}\")\n",
    "\n",
    "def process_all_jsonl_files(directory_path, db_conn):\n",
    "    jsonl_files = [f for f in os.listdir(directory_path) if f.endswith('.jsonl')]\n",
    "    for file_name in tqdm(jsonl_files, desc=\"Processing files\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            process_jsonl(file_path, db_conn)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    directory_path = os.getenv('JSONL_DIRECTORY_PATH', 'Datasets/oldtimey touringbot version 2/Svenska Dagbladet 1908-01-01-1908-12-31/KB Extracted Data')\n",
    "    db_path = os.getenv('DB_PATH', 'Datasets/oldtimey touringbot version 2/Svenska Dagbladet 1908-01-01-1908-12-31/OpenAI Results/completions.db')\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        logging.error(f\"Directory not found: {directory_path}\")\n",
    "        return\n",
    "\n",
    "    # Connect to the database\n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            # Create necessary tables\n",
    "            create_db_tables(conn)\n",
    "            \n",
    "            # Process all JSONL files in the directory\n",
    "            process_all_jsonl_files(directory_path, conn)\n",
    "            \n",
    "            # Optional: Perform any final database operations or checks\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM completions\")\n",
    "            completions_count = cursor.fetchone()[0]\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM events\")\n",
    "            events_count = cursor.fetchone()[0]\n",
    "            \n",
    "            logging.info(f\"Processing completed. Total completions: {completions_count}, Total events: {events_count}\")\n",
    "    \n",
    "    except sqlite3.Error as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
