{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching ComposedBlocks found.\n",
      "No matching ComposedBlocks found.\n",
      "Data exported to all_pages_output.xlsx\n",
      "Data exported to all_pages_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "class Page:\n",
    "    def __init__(self, xml_path=None, xml_content=None):\n",
    "        if xml_path:\n",
    "            self.load_xml_path(xml_path)\n",
    "        elif xml_content:\n",
    "            self.load_xml(xml_content)\n",
    "        else:\n",
    "            raise ValueError(\"No xml path or content provided.\")\n",
    "\n",
    "    def load_xml_path(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            xml = f.read()\n",
    "        self.load_xml(xml)\n",
    "\n",
    "    def load_xml(self, xml):\n",
    "        self.soup = bs(xml, features=\"xml\")\n",
    "\n",
    "    def sentence_from_keyword(self, keyword):\n",
    "        token = self.soup.find(\"String\", attrs={\"CONTENT\": keyword})\n",
    "        while token:\n",
    "            yield self.token_to_sentence(token)\n",
    "            token = token.find_next(\"String\", attrs={\"CONTENT\": keyword})\n",
    "\n",
    "    def token_to_sentence(self, token):\n",
    "        if token and token.parent:\n",
    "            strings = token.parent.find_all(\"String\")\n",
    "            return \" \".join(t[\"CONTENT\"] for t in strings)\n",
    "        return \"\"\n",
    "\n",
    "    def paragraph_from_keyword(self, keyword):\n",
    "        token = self.soup.find(\"String\", attrs={\"CONTENT\": keyword})\n",
    "        while token:\n",
    "            yield self.token_to_composed_block(token)\n",
    "            token = token.find_next(\"String\", attrs={\"CONTENT\": keyword})\n",
    "\n",
    "    def token_to_composed_block(self, token):\n",
    "        composed_block = token.find_parent(\"ComposedBlock\")\n",
    "        if composed_block:\n",
    "            text_blocks = composed_block.find_all(\"TextBlock\")\n",
    "            result = \"\"\n",
    "            for text_block in text_blocks:\n",
    "                text_lines = text_block.find_all(\"TextLine\")\n",
    "                for text_line in text_lines:\n",
    "                    strings = text_line.find_all(\"String\")\n",
    "                    line_content = \" \".join(string[\"CONTENT\"] for string in strings)\n",
    "                    result += f\"{line_content}\\n\"\n",
    "            return result.strip()\n",
    "        return \"\"\n",
    "\n",
    "    def article_from_keyword(self, keyword):\n",
    "        token = self.soup.find(\"String\", attrs={\"CONTENT\": keyword})\n",
    "        while token:\n",
    "            yield self.token_to_article(token)\n",
    "            token = token.find_next(\"String\", attrs={\"CONTENT\": keyword})\n",
    "\n",
    "    def token_to_article(self, token):\n",
    "        if token and token.parent and token.parent.parent and token.parent.parent.parent:\n",
    "            par_tags = token.parent.parent.parent.find_all(\"TextBlock\")\n",
    "            leading_tokens = (line_tag.find(\"String\") for line_tag in par_tags)\n",
    "            result = \"\"\n",
    "            for leading_token in leading_tokens:\n",
    "                paragraph = self.token_to_paragraph(leading_token)\n",
    "                result += f\"{paragraph}\\n\\n\"\n",
    "            return result.strip()\n",
    "        return \"\"\n",
    "\n",
    "def clean_json(text):\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def read_system_message():\n",
    "    try:\n",
    "        with open('oldtimey_touringbot_prompt_for_deployment.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return \"You are a helpful assistant.\"\n",
    "\n",
    "def row_to_json(row):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "    system_message_content = read_system_message()\n",
    "    system_message = {\"role\": \"system\", \"content\": system_message_content}\n",
    "\n",
    "    user_content_parts = [str(row[col]) for col in row.index if col not in ['System Content', 'Package ID', 'Part', 'Page']]\n",
    "    user_message = {\"role\": \"user\", \"content\": \" \".join(user_content_parts)}\n",
    "\n",
    "    custom_id = f\"{row['Package ID']}-{row['Part']}-{row['Page']}-{counter}\"\n",
    "\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo-0125\",\n",
    "            \"messages\": [system_message, user_message],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    }\n",
    "\n",
    "def search_swedish_newspapers(to_date, from_date, collection_id, query):\n",
    "    base_url = 'https://data.kb.se/search'\n",
    "    params = {\n",
    "        'to': to_date,\n",
    "        'from': from_date,\n",
    "        'isPartOf.@id': collection_id,\n",
    "        'q': query,\n",
    "        'searchGranularity': 'part'\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except ValueError:\n",
    "            return {'error': 'Invalid JSON response'}\n",
    "    else:\n",
    "        return {'error': response.status_code, 'message': response.text}\n",
    "\n",
    "def extract_urls(result):\n",
    "    base_url = 'https://data.kb.se'\n",
    "    details = []\n",
    "\n",
    "    for hit in result['hits']:\n",
    "        part_number = hit.get('part')\n",
    "        page_number = hit.get('page')\n",
    "        package_id = hit.get('hasFilePackage', {}).get('@id', '').split('/')[-1]\n",
    "\n",
    "        if part_number and page_number and package_id:\n",
    "            url = f\"{base_url}/{package_id}/part/{part_number}/page/{page_number}\"\n",
    "            details.append({\n",
    "                'part_number': part_number,\n",
    "                'page_number': page_number,\n",
    "                'package_id': package_id,\n",
    "                'url': url\n",
    "            })\n",
    "\n",
    "    return details\n",
    "\n",
    "def extract_xml_urls(api_response, page_numbers=None):\n",
    "    xml_urls = {}\n",
    "    parts_list = api_response.get('hasPart', [])\n",
    "\n",
    "    if page_numbers is not None:\n",
    "        page_numbers = [int(page) for page in page_numbers]\n",
    "\n",
    "    for part in parts_list:\n",
    "        pages_list = part.get('hasPartList', [])\n",
    "        for page in pages_list:\n",
    "            page_id = page['@id']\n",
    "            page_number = int(page_id.split('/')[-1].replace('page', ''))\n",
    "\n",
    "            if page_numbers is None or page_number in page_numbers:\n",
    "                includes_list = page.get('includes', [])\n",
    "                for include in includes_list:\n",
    "                    if include['@id'].endswith('alto.xml'):\n",
    "                        xml_urls[page_number] = include['@id']\n",
    "\n",
    "    return xml_urls\n",
    "\n",
    "def fetch_xml_content(xml_urls_by_package):\n",
    "    xml_content_by_package = {}\n",
    "\n",
    "    for package_id, parts in xml_urls_by_package.items():\n",
    "        for part_number, xml_urls in parts.items():\n",
    "            if package_id not in xml_content_by_package:\n",
    "                xml_content_by_package[package_id] = {}\n",
    "\n",
    "            if part_number not in xml_content_by_package[package_id]:\n",
    "                xml_content_by_package[package_id][part_number] = {}\n",
    "\n",
    "            for page_number, url in xml_urls.items():\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    xml_content_by_package[package_id][part_number][page_number] = response.content\n",
    "                else:\n",
    "                    print(f\"Failed to fetch XML content from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "    return xml_content_by_package\n",
    "\n",
    "def main():\n",
    "    from_date = '1908-01-01'\n",
    "    to_date = '1908-01-03'\n",
    "    collection_id = 'https://libris.kb.se/2ldhmx8d4mcrlq9#it'  # Svenska dagbladet\n",
    "    query = 'konsert'\n",
    "\n",
    "    result = search_swedish_newspapers(to_date, from_date, collection_id, query)\n",
    "    if 'error' in result:\n",
    "        print(f\"Search error: {result['error']}\")\n",
    "        return\n",
    "\n",
    "    detailed_info = extract_urls(result)\n",
    "    if not detailed_info:\n",
    "        print(\"No URLs found.\")\n",
    "        return\n",
    "\n",
    "    api_responses = []\n",
    "    for info in detailed_info:\n",
    "        url = info['url']\n",
    "        response = requests.get(url, headers={'Accept': 'application/json'})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            api_responses.append((data, info))\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "    xml_urls_by_package = {}\n",
    "    for data, info in api_responses:\n",
    "        package_id = info['package_id']\n",
    "        part_number = info['part_number']\n",
    "        page_numbers = [info['page_number']]\n",
    "        xml_urls = extract_xml_urls(data, page_numbers)\n",
    "        if package_id not in xml_urls_by_package:\n",
    "            xml_urls_by_package[package_id] = {}\n",
    "        if part_number not in xml_urls_by_package[package_id]:\n",
    "            xml_urls_by_package[package_id][part_number] = {}\n",
    "        xml_urls_by_package[package_id][part_number].update(xml_urls)\n",
    "\n",
    "    xml_content_by_package = fetch_xml_content(xml_urls_by_package)\n",
    "\n",
    "    def get_xml_content(package_id, part_number, page_number):\n",
    "        return xml_content_by_package.get(package_id, {}).get(part_number, {}).get(page_number, None)\n",
    "\n",
    "    all_data_frames = []\n",
    "\n",
    "    for info in detailed_info:\n",
    "        package_id = info['package_id']\n",
    "        part_number = info['part_number']\n",
    "        page_number = info['page_number']\n",
    "\n",
    "        retrieved_xml_content = get_xml_content(package_id, part_number, page_number)\n",
    "        if not retrieved_xml_content:\n",
    "            print(f\"No XML content found for Package ID: {package_id}, Part: {part_number}, Page: {page_number}\")\n",
    "            continue\n",
    "\n",
    "        xml_string = retrieved_xml_content.decode('utf-8')\n",
    "        page = Page(xml_content=xml_string)\n",
    "        formatted_date = extract_and_format_date(page)\n",
    "        if not formatted_date:\n",
    "            print(\"Date extraction failed.\")\n",
    "            continue\n",
    "\n",
    "        matching_composed_blocks = extract_textblocks(page, query)\n",
    "        if not matching_composed_blocks:\n",
    "            print(\"No matching ComposedBlocks found.\")\n",
    "            continue\n",
    "\n",
    "        contents = extract_textblock_content(matching_composed_blocks)\n",
    "\n",
    "        df = pd.DataFrame(contents, columns=[\"ComposedBlock Content\"])\n",
    "        df['Date'] = formatted_date\n",
    "        df['Package ID'] = package_id\n",
    "        df['Part'] = part_number\n",
    "        df['Page'] = page_number\n",
    "\n",
    "        all_data_frames.append(df)\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        output_xls_path = \"all_pages_output.xlsx\"\n",
    "        output_jsonl_path = \"all_pages_output.jsonl\"\n",
    "\n",
    "        final_df.to_excel(output_xls_path, index=False)\n",
    "        print(f\"Data exported to {output_xls_path}\")\n",
    "\n",
    "        with open(output_jsonl_path, 'w') as jsonl_file:\n",
    "            for _, row in final_df.iterrows():\n",
    "                jsonl_file.write(json.dumps(row_to_json(row)) + '\\n')\n",
    "        print(f\"Data exported to {output_jsonl_path}\")\n",
    "    else:\n",
    "        print(\"No data to export.\")\n",
    "\n",
    "def extract_and_format_date(page):\n",
    "    file_name_element = page.soup.find('fileName')\n",
    "    if file_name_element:\n",
    "        file_name = file_name_element.text\n",
    "        date_match = re.search(r'_(\\d{8})_', file_name)\n",
    "        if date_match:\n",
    "            date_str = date_match.group(1)\n",
    "            formatted_date = f\"{date_str[6:8]}.{date_str[4:6]}.{date_str[0:4]}\"\n",
    "            return formatted_date\n",
    "    return None\n",
    "\n",
    "def extract_textblocks(page, keyword):\n",
    "    matching_composed_blocks = []\n",
    "    for composed_block in page.paragraph_from_keyword(keyword):\n",
    "        matching_composed_blocks.append(composed_block)\n",
    "    return matching_composed_blocks\n",
    "\n",
    "def extract_textblock_content(matching_composed_blocks):\n",
    "    contents = []\n",
    "    for composed_block_content in matching_composed_blocks:\n",
    "        contents.append(composed_block_content.strip())\n",
    "    return contents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
