{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits found: 4\n",
      "Processing XML content for Package ID: dark-77536, Part: 1, Page: 12\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_parent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 297\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m contents\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 241\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate extraction failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m matching_composed_blocks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_textblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matching_composed_blocks:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo matching ComposedBlocks found in Package ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Part: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Page: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 284\u001b[0m, in \u001b[0;36mextract_textblocks\u001b[0;34m(page, keyword)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_textblocks\u001b[39m(page, keyword):\n\u001b[1;32m    283\u001b[0m     matching_composed_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomposed_block\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparagraph_from_keyword\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomposed_block\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Check if composed block is not None\u001b[39;49;00m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMatching ComposedBlock found with keyword \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkeyword\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcomposed_block\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mPage.paragraph_from_keyword\u001b[0;34m(self, keyword)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparagraph_from_keyword\u001b[39m(\u001b[38;5;28mself\u001b[39m, keyword):\n\u001b[1;32m     35\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mString\u001b[39m\u001b[38;5;124m\"\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTENT\u001b[39m\u001b[38;5;124m\"\u001b[39m: keyword})\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_to_composed_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (token \u001b[38;5;241m:=\u001b[39m token\u001b[38;5;241m.\u001b[39mfind_next(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mString\u001b[39m\u001b[38;5;124m\"\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTENT\u001b[39m\u001b[38;5;124m\"\u001b[39m: keyword})) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_to_composed_block(token)\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mPage.token_to_composed_block\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken_to_composed_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[0;32m---> 41\u001b[0m     composed_block \u001b[38;5;241m=\u001b[39m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_parent\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComposedBlock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m composed_block:\n\u001b[1;32m     43\u001b[0m         text_blocks \u001b[38;5;241m=\u001b[39m composed_block\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextBlock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_parent'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "class Page:\n",
    "    def __init__(self, xml_path=None, xml_content=None):\n",
    "        if xml_path:\n",
    "            self.load_xml_path(xml_path)\n",
    "        elif xml_content:\n",
    "            self.load_xml(xml_content)\n",
    "        else:\n",
    "            raise ValueError(\"No xml path or content provided.\")\n",
    "\n",
    "    def load_xml_path(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            xml = f.read()\n",
    "        self.load_xml(xml)\n",
    "\n",
    "    def load_xml(self, xml):\n",
    "        self.soup = bs(xml, features=\"xml\")\n",
    "\n",
    "    def sentence_from_keyword(self, keyword):\n",
    "        token = self.soup.find(\"String\", attrs={\"CONTENT\": keyword})\n",
    "        yield self.token_to_sentence(token)\n",
    "        while (token := token.find_next(\"String\", attrs={\"CONTENT\": keyword})) is not None:\n",
    "            yield self.token_to_sentence(token)\n",
    "\n",
    "    def token_to_sentence(self, token):\n",
    "        strings = token.parent.find_all(\"String\")\n",
    "        return \" \".join(t[\"CONTENT\"] for t in strings)\n",
    "\n",
    "    def paragraph_from_keyword(self, keyword):\n",
    "        token = self.soup.find(\"String\", attrs={\"CONTENT\": keyword})\n",
    "        yield self.token_to_composed_block(token)\n",
    "        while (token := token.find_next(\"String\", attrs={\"CONTENT\": keyword})) is not None:\n",
    "            yield self.token_to_composed_block(token)\n",
    "\n",
    "    def token_to_composed_block(self, token):\n",
    "        composed_block = token.find_parent(\"ComposedBlock\")\n",
    "        if composed_block:\n",
    "            text_blocks = composed_block.find_all(\"TextBlock\")\n",
    "            result = \"\"\n",
    "            for text_block in text_blocks:\n",
    "                text_lines = text_block.find_all(\"TextLine\")\n",
    "                for text_line in text_lines:\n",
    "                    strings = text_line.find_all(\"String\")\n",
    "                    line_content = \" \".join(string[\"CONTENT\"] for string in strings)\n",
    "                    result += f\"{line_content}\\n\"\n",
    "            return result.strip()\n",
    "        return \"\"\n",
    "\n",
    "def clean_json(text):\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def read_system_message():\n",
    "    try:\n",
    "        with open('oldtimey_touringbot_prompt_for_deployment.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return \"You are a helpful assistant.\"\n",
    "\n",
    "def row_to_json(row):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "    system_message_content = read_system_message()\n",
    "    system_message = {\"role\": \"system\", \"content\": system_message_content}\n",
    "\n",
    "    user_content_parts = [str(row[col]) for col in row.index if col not in ['System Content', 'Package ID', 'Part', 'Page']]\n",
    "    user_message = {\"role\": \"user\", \"content\": \" \".join(user_content_parts)}\n",
    "\n",
    "    custom_id = f\"{row['Package ID']}-{row['Part']}-{row['Page']}-{counter}\"\n",
    "\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo-0125\",\n",
    "            \"messages\": [system_message, user_message],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    }\n",
    "\n",
    "def search_swedish_newspapers(to_date, from_date, collection_id, query):\n",
    "    base_url = 'https://data.kb.se/search'\n",
    "    params = {\n",
    "        'to': to_date,\n",
    "        'from': from_date,\n",
    "        'isPartOf.@id': collection_id,\n",
    "        'q': query,\n",
    "        'searchGranularity': 'part'\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except ValueError:\n",
    "            return {'error': 'Invalid JSON response'}\n",
    "    else:\n",
    "        return {'error': response.status_code, 'message': response.text}\n",
    "\n",
    "def extract_urls(result):\n",
    "    base_url = 'https://data.kb.se'\n",
    "    details = []\n",
    "\n",
    "    for hit in result['hits']:\n",
    "        part_number = hit.get('part')\n",
    "        page_number = hit.get('page')\n",
    "        package_id = hit.get('hasFilePackage', {}).get('@id', '').split('/')[-1]\n",
    "\n",
    "        if part_number and page_number and package_id:\n",
    "            url = f\"{base_url}/{package_id}/part/{part_number}/page/{page_number}\"\n",
    "            details.append({\n",
    "                'part_number': part_number,\n",
    "                'page_number': page_number,\n",
    "                'package_id': package_id,\n",
    "                'url': url\n",
    "            })\n",
    "\n",
    "    return details\n",
    "\n",
    "def extract_xml_urls(api_response, page_numbers=None):\n",
    "    xml_urls = {}\n",
    "    parts_list = api_response.get('hasPart', [])\n",
    "\n",
    "    if page_numbers is not None:\n",
    "        page_numbers = [int(page) for page in page_numbers]\n",
    "\n",
    "    for part in parts_list:\n",
    "        pages_list = part.get('hasPartList', [])\n",
    "        for page in pages_list:\n",
    "            page_id = page['@id']\n",
    "            page_number = int(page_id.split('/')[-1].replace('page', ''))\n",
    "\n",
    "            if page_numbers is None or page_number in page_numbers:\n",
    "                includes_list = page.get('includes', [])\n",
    "                for include in includes_list:\n",
    "                    if include['@id'].endswith('alto.xml'):\n",
    "                        xml_urls[page_number] = include['@id']\n",
    "\n",
    "    return xml_urls\n",
    "\n",
    "def fetch_xml_content(xml_urls_by_package):\n",
    "    xml_content_by_package = {}\n",
    "\n",
    "    for package_id, parts in xml_urls_by_package.items():\n",
    "        for part_number, xml_urls in parts.items():\n",
    "            if package_id not in xml_content_by_package:\n",
    "                xml_content_by_package[package_id] = {}\n",
    "\n",
    "            if part_number not in xml_content_by_package[package_id]:\n",
    "                xml_content_by_package[package_id][part_number] = {}\n",
    "\n",
    "            for page_number, url in xml_urls.items():\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    xml_content_by_package[package_id][part_number][page_number] = response.content\n",
    "                else:\n",
    "                    print(f\"Failed to fetch XML content from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "    return xml_content_by_package\n",
    "\n",
    "def main():\n",
    "    from_date = '1908-01-01'\n",
    "    to_date = '1908-01-03'\n",
    "    collection_id = 'https://libris.kb.se/2ldhmx8d4mcrlq9#it'  # Svenska dagbladet\n",
    "    query = 'konsert'\n",
    "\n",
    "    result = search_swedish_newspapers(to_date, from_date, collection_id, query)\n",
    "    if 'error' in result:\n",
    "        print(f\"Search error: {result['error']}\")\n",
    "        return\n",
    "\n",
    "    detailed_info = extract_urls(result)\n",
    "    if not detailed_info:\n",
    "        print(\"No URLs found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total hits found: {len(detailed_info)}\")\n",
    "\n",
    "    api_responses = []\n",
    "    for info in detailed_info:\n",
    "        url = info['url']\n",
    "        response = requests.get(url, headers={'Accept': 'application/json'})\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            api_responses.append((data, info))\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "    xml_urls_by_package = {}\n",
    "    for data, info in api_responses:\n",
    "        package_id = info['package_id']\n",
    "        part_number = info['part_number']\n",
    "        page_numbers = [info['page_number']]\n",
    "        xml_urls = extract_xml_urls(data, page_numbers)\n",
    "        if package_id not in xml_urls_by_package:\n",
    "            xml_urls_by_package[package_id] = {}\n",
    "        if part_number not in xml_urls_by_package[package_id]:\n",
    "            xml_urls_by_package[package_id][part_number] = {}\n",
    "        xml_urls_by_package[package_id][part_number].update(xml_urls)\n",
    "\n",
    "    xml_content_by_package = fetch_xml_content(xml_urls_by_package)\n",
    "\n",
    "    def get_xml_content(package_id, part_number, page_number):\n",
    "        return xml_content_by_package.get(package_id, {}).get(part_number, {}).get(page_number, None)\n",
    "\n",
    "    all_data_frames = []\n",
    "\n",
    "    for info in detailed_info:\n",
    "        package_id = info['package_id']\n",
    "        part_number = info['part_number']\n",
    "        page_number = info['page_number']\n",
    "\n",
    "        retrieved_xml_content = get_xml_content(package_id, part_number, page_number)\n",
    "        if not retrieved_xml_content:\n",
    "            print(f\"No XML content found for Package ID: {package_id}, Part: {part_number}, Page: {page_number}\")\n",
    "            continue\n",
    "\n",
    "        xml_string = retrieved_xml_content.decode('utf-8')\n",
    "        page = Page(xml_content=xml_string)\n",
    "        \n",
    "        print(f\"Processing XML content for Package ID: {package_id}, Part: {part_number}, Page: {page_number}\")\n",
    "        \n",
    "        formatted_date = extract_and_format_date(page)\n",
    "        if not formatted_date:\n",
    "            print(\"Date extraction failed.\")\n",
    "            continue\n",
    "\n",
    "        matching_composed_blocks = extract_textblocks(page, query)\n",
    "        if not matching_composed_blocks:\n",
    "            print(f\"No matching ComposedBlocks found in Package ID: {package_id}, Part: {part_number}, Page: {page_number}\")\n",
    "            continue\n",
    "\n",
    "        contents = extract_textblock_content(matching_composed_blocks)\n",
    "\n",
    "        df = pd.DataFrame(contents, columns=[\"ComposedBlock Content\"])\n",
    "        df['Date'] = formatted_date\n",
    "        df['Package ID'] = package_id\n",
    "        df['Part'] = part_number\n",
    "        df['Page'] = page_number\n",
    "\n",
    "        all_data_frames.append(df)\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        output_xls_path = \"all_pages_output.xlsx\"\n",
    "        output_jsonl_path = \"all_pages_output.jsonl\"\n",
    "\n",
    "        final_df.to_excel(output_xls_path, index=False)\n",
    "        print(f\"Data exported to {output_xls_path}\")\n",
    "\n",
    "        with open(output_jsonl_path, 'w') as jsonl_file:\n",
    "            for _, row in final_df.iterrows():\n",
    "                jsonl_file.write(json.dumps(row_to_json(row)) + '\\n')\n",
    "        print(f\"Data exported to {output_jsonl_path}\")\n",
    "    else:\n",
    "        print(\"No data to export.\")\n",
    "\n",
    "def extract_and_format_date(page):\n",
    "    file_name_element = page.soup.find('fileName')\n",
    "    if file_name_element:\n",
    "        file_name = file_name_element.text\n",
    "        date_match = re.search(r'_(\\d{8})_', file_name)\n",
    "        if date_match:\n",
    "            date_str = date_match.group(1)\n",
    "            formatted_date = f\"{date_str[6:8]}.{date_str[4:6]}.{date_str[0:4]}\"\n",
    "            return formatted_date\n",
    "    return None\n",
    "\n",
    "def extract_textblocks(page, keyword):\n",
    "    matching_composed_blocks = []\n",
    "    for composed_block in page.paragraph_from_keyword(keyword):\n",
    "        if composed_block:  # Check if composed block is not None\n",
    "            print(f\"Matching ComposedBlock found with keyword '{keyword}': {composed_block}\")\n",
    "            matching_composed_blocks.append(composed_block)\n",
    "    return matching_composed_blocks\n",
    "\n",
    "def extract_textblock_content(matching_composed_blocks):\n",
    "    contents = []\n",
    "    for composed_block_content in matching_composed_blocks:\n",
    "        contents.append(composed_block_content.strip())\n",
    "    return contents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
