{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def search_swedish_newspapers(to_date, from_date, collection_id, query):\n",
    "    base_url = 'https://data.kb.se/search'\n",
    "    params = {\n",
    "        'to': to_date,\n",
    "        'from': from_date,\n",
    "        'isPartOf.@id': collection_id,\n",
    "        'q': query,\n",
    "        'searchGranularity': 'part'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except ValueError:\n",
    "            return {'error': 'Invalid JSON response'}\n",
    "    else:\n",
    "        return {'error': response.status_code, 'message': response.text}\n",
    "\n",
    "\n",
    "from_date = '1908-01-01'\n",
    "to_date = '1908-02-29'\n",
    "\n",
    "collection_id = 'https://libris.kb.se/2ldhmx8d4mcrlq9#it' #Svenska dagbladet. Can you add others? probably.\n",
    "#collection_id = 'https://libris.kb.se/m5z2w4lz3m2zxpk#it' # Dagens nyheter. Does this work? nope. I think the xml structure is different so it fails at that step. oh well\n",
    " \n",
    "query = 'konsert'\n",
    "result = search_swedish_newspapers(to_date, from_date, collection_id, query)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get the XML file URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(result):\n",
    "    base_url = 'https://data.kb.se'\n",
    "    details = []\n",
    "\n",
    "    for hit in result['hits']:\n",
    "        part_number = hit.get('part')\n",
    "        page_number = hit.get('page')\n",
    "        package_id = hit.get('hasFilePackage', {}).get('@id', '').split('/')[-1]\n",
    "\n",
    "        if part_number and page_number and package_id:\n",
    "            url = f\"{base_url}/{package_id}/part/{part_number}/page/{page_number}\"\n",
    "            details.append({\n",
    "                'part_number': part_number,\n",
    "                'page_number': page_number,\n",
    "                'package_id': package_id,\n",
    "                'url': url\n",
    "            })\n",
    "    \n",
    "    return details\n",
    "\n",
    "# Get the URLs\n",
    "detailed_info = extract_urls(result)\n",
    "for info in detailed_info:\n",
    "    print(f\"Package ID: {info['package_id']}, Part: {info['part_number']}, Page: {info['page_number']}, URL: {info['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next is getting the XML files from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Accept': 'application/json'}\n",
    "\n",
    "# Assuming 'detailed_info' is the output from the modified 'extract_urls' function\n",
    "api_responses = []\n",
    "for info in detailed_info:\n",
    "    url = info['url']  # URL to send the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        api_responses.append(data)\n",
    "        print(f\"Data from Package ID: {info['package_id']}, Part: {info['part_number']}, Page: {info['page_number']}, URL: {url}:\")\n",
    "        print(data)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to extract XML URLs from API response\n",
    "def extract_xml_urls(api_response, page_numbers=None):\n",
    "    xml_urls = {}\n",
    "    parts_list = api_response.get('hasPart', [])\n",
    "\n",
    "    # Convert page_numbers from string to integer for proper comparison\n",
    "    if page_numbers is not None:\n",
    "        page_numbers = [int(page) for page in page_numbers]  # Ensure page_numbers are integers\n",
    "\n",
    "    for part in parts_list:\n",
    "        pages_list = part.get('hasPartList', [])\n",
    "        for page in pages_list:\n",
    "            page_id = page['@id']\n",
    "            page_number = int(page_id.split('/')[-1].replace('page', ''))  # Ensure page_number is an integer\n",
    "\n",
    "            # Only process pages that are either not filtered by page_numbers or are within the specified list\n",
    "            if page_numbers is None or page_number in page_numbers:\n",
    "                includes_list = page.get('includes', [])\n",
    "                for include in includes_list:\n",
    "                    if include['@id'].endswith('alto.xml'):\n",
    "                        xml_urls[page_number] = include['@id']  # Use page_number as an int key\n",
    "\n",
    "    return xml_urls\n",
    "\n",
    "# Function to fetch and store XML content from URLs\n",
    "def fetch_xml_content(xml_urls_by_package):\n",
    "    xml_content_by_package = {}\n",
    "\n",
    "    for package_id, parts in xml_urls_by_package.items():\n",
    "        for part_number, xml_urls in parts.items():\n",
    "            if package_id not in xml_content_by_package:\n",
    "                xml_content_by_package[package_id] = {}\n",
    "\n",
    "            if part_number not in xml_content_by_package[package_id]:\n",
    "                xml_content_by_package[package_id][part_number] = {}\n",
    "\n",
    "            for page_number, url in xml_urls.items():\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    xml_content_by_package[package_id][part_number][page_number] = response.content\n",
    "                else:\n",
    "                    print(f\"Failed to fetch XML content from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "    return xml_content_by_package\n",
    "\n",
    "\n",
    "headers = {'Accept': 'application/json'}\n",
    "api_responses = []\n",
    "\n",
    "# Loop through each item in detailed_info to fetch data from URLs\n",
    "for info in detailed_info:\n",
    "    url = info['url']  # URL to send the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        api_responses.append((data, info))  # Store response and corresponding info together\n",
    "        print(f\"Data from Package ID: {info['package_id']}, Part: {info['part_number']}, Page: {info['page_number']}, URL: {url}:\")\n",
    "        print(data)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "# Dictionary to store XML URLs by package ID and part number\n",
    "xml_urls_by_package = {}\n",
    "\n",
    "# Extract and store XML URLs from the collected API responses\n",
    "for data, info in api_responses:\n",
    "    package_id = info['package_id']\n",
    "    part_number = info['part_number']\n",
    "    page_numbers = [info['page_number']]\n",
    "    xml_urls = extract_xml_urls(data, page_numbers)\n",
    "\n",
    "    if package_id not in xml_urls_by_package:\n",
    "        xml_urls_by_package[package_id] = {}\n",
    "\n",
    "    if part_number not in xml_urls_by_package[package_id]:\n",
    "        xml_urls_by_package[package_id][part_number] = {}\n",
    "\n",
    "    xml_urls_by_package[package_id][part_number].update(xml_urls)\n",
    "\n",
    "# Fetch and store XML content\n",
    "xml_content_by_package = fetch_xml_content(xml_urls_by_package)\n",
    "\n",
    "# Example function to access the stored XML content\n",
    "def get_xml_content(package_id, part_number, page_number):\n",
    "    return xml_content_by_package.get(package_id, {}).get(part_number, {}).get(page_number, None)\n",
    "\n",
    "# Example usage of the get_xml_content function\n",
    "#package_id = 'dark-77531'\n",
    "#part_number = 1\n",
    "#page_number = 12\n",
    "#retrieved_xml_content = get_xml_content(package_id, part_number, page_number)\n",
    "#if retrieved_xml_content:\n",
    "    print(f\"Retrieved XML content for Package ID: {package_id}, Part: {part_number}, Page: {page_number}:\")\n",
    "    print(retrieved_xml_content.decode('utf-8'))\n",
    "#else:\n",
    "    print(f\"No XML content found for Package ID: {package_id}, Part: {part_number}, Page: {page_number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now we're smashing this together with the Concert XML converter. What this converter does is take each match and extract the text blocks around the matching words. Wow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is now the text processing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set prompt instructions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import xml_from_matthias\n",
    "\n",
    "def clean_json(text):\n",
    "    # Implement or import your clean_json function that potentially cleans the text content\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "# Initialize the counter at the top level of the script\n",
    "counter = 0\n",
    "\n",
    "def read_system_message():\n",
    "    # Try to read the content of the text file and return it\n",
    "    try:\n",
    "        with open('oldtimey_touringbot_prompt_for_deployment.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return \"You are a helpful assistant.\"  # Fallback message if file does not exist\n",
    "\n",
    "def row_to_json(row):\n",
    "    global counter  # Declare the use of the global variable\n",
    "    counter += 1  # Increment the counter with each call to the function\n",
    "    \n",
    "    # Get system message from text file\n",
    "    system_message_content = read_system_message()\n",
    "    system_message = {\"role\": \"system\", \"content\": system_message_content}\n",
    "    \n",
    "    # Dynamic user content, concatenating all relevant DataFrame columns\n",
    "    user_content_parts = [str(row[col]) for col in row.index if col not in ['System Content', 'Package ID', 'Part', 'Page']]  # Exclude ID columns and other specific columns as needed\n",
    "    user_message = {\"role\": \"user\", \"content\": \" \".join(user_content_parts)}\n",
    "    \n",
    "    # Constructing custom_id from Package ID, Part Number, Page Number, and the global counter\n",
    "    custom_id = f\"{row['Package ID']}-{row['Part']}-{row['Page']}-{counter}\"\n",
    "    \n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo-0125\",  # Static model name\n",
    "            \"messages\": [system_message, user_message],\n",
    "            \"max_tokens\": 1000  # Static maximum token limit\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Function to load the XML content from a string\n",
    "def load_xml_from_string(xml_string):\n",
    "    try:\n",
    "        page = Page(xml_content=xml_string)\n",
    "        return page\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading XML content: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract the date from the <fileName> tag and format it\n",
    "def extract_and_format_date(page):\n",
    "    file_name_element = page.soup.find('fileName')\n",
    "    if file_name_element:\n",
    "        file_name = file_name_element.text\n",
    "        # Extract date using regex\n",
    "        date_match = re.search(r'_(\\d{8})_', file_name)\n",
    "        if date_match:\n",
    "            date_str = date_match.group(1)\n",
    "            # Convert YYYYMMDD to DD.MM.YYYY\n",
    "            formatted_date = f\"{date_str[6:8]}.{date_str[4:6]}.{date_str[0:4]}\"\n",
    "            return formatted_date\n",
    "    return None\n",
    "\n",
    "# Function to search for keyword and extract <TextBlock> elements, including context\n",
    "def extract_textblocks(page, keyword, context_range=1):\n",
    "    matching_textblocks = []\n",
    "    for paragraph in page.paragraph_from_keyword(keyword):\n",
    "        matching_textblocks.append(paragraph)\n",
    "    return matching_textblocks\n",
    "\n",
    "# Function to extract the CONTENT from <String> elements in <TextBlock>\n",
    "def extract_textblock_content(matching_textblocks):\n",
    "    contents = []\n",
    "    for textblock in matching_textblocks:\n",
    "        contents.append(textblock.strip())\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Assuming these functions are already defined as per your previous instructions\n",
    "def clean_json(text):\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def read_system_message():\n",
    "    try:\n",
    "        with open('oldtimey_touringbot_prompt_for_deployment.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return \"You are a helpful assistant.\"\n",
    "\n",
    "def row_to_json(row):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    \n",
    "    system_message_content = read_system_message()\n",
    "    system_message = {\"role\": \"system\", \"content\": system_message_content}\n",
    "    \n",
    "    user_content_parts = [str(row[col]) for col in row.index if col not in ['System Content', 'Package ID', 'Part', 'Page']]\n",
    "    user_message = {\"role\": \"user\", \"content\": \" \".join(user_content_parts)}\n",
    "    \n",
    "    custom_id = f\"{row['Package ID']}-{row['Part']}-{row['Page']}-{counter}\"\n",
    "    \n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo-0125\",\n",
    "            \"messages\": [system_message, user_message],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # For testing purposes, the query is hardcoded; replace with actual query input as needed\n",
    "    query = \"your_keyword_here\"  \n",
    "    context_range = 5\n",
    "\n",
    "    if not query:\n",
    "        print(\"No keyword entered. Exiting.\")\n",
    "        return\n",
    "\n",
    "    all_data_frames = []\n",
    "\n",
    "    detailed_info = [\n",
    "        {'package_id': 'package1', 'part_number': 'part1', 'page_number': 'page1'},\n",
    "        {'package_id': 'package2', 'part_number': 'part2', 'page_number': 'page2'}\n",
    "        # Add more entries as needed\n",
    "    ]\n",
    "\n",
    "    for info in detailed_info:\n",
    "        package_id = info['package_id']\n",
    "        part_number = info['part_number']\n",
    "        page_number = info['page_number']\n",
    "\n",
    "        retrieved_xml_content = get_xml_content(package_id, part_number, page_number)\n",
    "        if not retrieved_xml_content:\n",
    "            print(f\"No XML content found for Package ID: {package_id}, Part: {part_number}, Page: {page_number}\")\n",
    "            continue\n",
    "\n",
    "        xml_string = retrieved_xml_content.decode('utf-8')\n",
    "        xml_root = load_xml_from_string(xml_string)\n",
    "        if xml_root is None:\n",
    "            continue\n",
    "\n",
    "        formatted_date = extract_and_format_date(xml_root)\n",
    "        if not formatted_date:\n",
    "            print(\"Date extraction failed.\")\n",
    "            continue\n",
    "\n",
    "        matching_textblocks = extract_textblocks(xml_root, query, context_range)\n",
    "        if not matching_textblocks:\n",
    "            print(\"No matching TextBlocks found.\")\n",
    "            continue\n",
    "\n",
    "        contents = extract_textblock_content(matching_textblocks)\n",
    "\n",
    "        df = pd.DataFrame(contents, columns=[\"TextBlock Content\"])\n",
    "        df['Date'] = formatted_date\n",
    "        df['Package ID'] = package_id\n",
    "        df['Part'] = part_number\n",
    "        df['Page'] = page_number\n",
    "\n",
    "        all_data_frames.append(df)\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        output_xls_path = \"all_pages_output.xlsx\"\n",
    "        output_jsonl_path = \"all_pages_output.jsonl\"\n",
    "\n",
    "        final_df.to_excel(output_xls_path, index=False)\n",
    "        print(f\"Data exported to {output_xls_path}\")\n",
    "\n",
    "        with open(output_jsonl_path, 'w') as jsonl_file:\n",
    "            for _, row in final_df.iterrows():\n",
    "                jsonl_file.write(json.dumps(row_to_json(row)) + '\\n')\n",
    "        print(f\"Data exported to {output_jsonl_path}\")\n",
    "    else:\n",
    "        print(\"No data to export.\")\n",
    "\n",
    "def get_xml_content(package_id, part_number, page_number):\n",
    "    # Simulate retrieval of XML content for testing\n",
    "    # Replace with actual logic to get XML content\n",
    "    return b\"\"\"<root>\n",
    "                <fileName>test_20220101_01.xml</fileName>\n",
    "                <TextBlock>\n",
    "                    <TextLine>\n",
    "                        <String CONTENT=\"your_keyword_here\"/>\n",
    "                    </TextLine>\n",
    "                </TextBlock>\n",
    "              </root>\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
