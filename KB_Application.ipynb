{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file path: /Users/brandonfarnsworth/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Published Texts/Method Article Collecting Trace Data using LLM/Lokal_Datablad_Original.xlsx\n",
      "Today's date: 2024-06-29\n",
      "Start year for data fetching: 1908\n",
      "Years to crawl: 1\n",
      "Name of newspaper: Svenska Dagbladet\n",
      "Valid options for newspapers: ['Dagens nyheter', 'Svenska Dagbladet', 'Aftonbladet', 'Dagligt Allehanda']\n",
      "KB API Max Retries: 10\n",
      "KB API Max Retry Time: 300\n",
      "\n",
      "DataFrame preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lokal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Croix salong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norra paviljongen i Trädgårdsföreningens lokal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wallmans lokal (Mäster Samuels gränd 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kungliga operan/Kungliga teatern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F.d. Kirsteinska huset (vid Clara) [Hotel W6 r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Lokal\n",
       "0                                    La Croix salong\n",
       "1     Norra paviljongen i Trädgårdsföreningens lokal\n",
       "2           Wallmans lokal (Mäster Samuels gränd 11)\n",
       "3                   Kungliga operan/Kungliga teatern\n",
       "4  F.d. Kirsteinska huset (vid Clara) [Hotel W6 r..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "import requests\n",
    "from KBDownloader import fetch_newspaper_data\n",
    "\n",
    "# Load configuration\n",
    "def load_config(config_path='config.yaml'):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(config['excel_filepath'])\n",
    "\n",
    "# Get today's date\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Setup backoff decorator\n",
    "@backoff.on_exception(backoff.expo, \n",
    "                      (requests.exceptions.RequestException, requests.exceptions.HTTPError),\n",
    "                      max_tries=config['max_retries'],\n",
    "                      max_time=config['max_retry_time'])\n",
    "def fetch_with_backoff(*args, **kwargs):\n",
    "    return fetch_newspaper_data(*args, **kwargs)\n",
    "\n",
    "# Print out variables and assumptions\n",
    "print(f\"Excel file path: {config['excel_filepath']}\")\n",
    "print(f\"Today's date: {today_date}\")\n",
    "print(f\"Start year for data fetching: {config['start_year']}\")\n",
    "print(f\"Years to crawl: {config['years_to_crawl']}\")\n",
    "print(f\"Name of newspaper: {config['newspaper']}\")\n",
    "print(f\"Valid options for newspapers: {config['valid_newspapers']}\")\n",
    "print(f\"KB API Max Retries: {config['max_retries']}\")\n",
    "print(f\"KB API Max Retry Time: {config['max_retry_time']}\")\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify loading\n",
    "print(\"\\nDataFrame preview:\")\n",
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Iteratively go through the venues (Lokale) and return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch XML content from https://data.kb.se/dark-78626/bib13434192_19080524_11631_140_0011_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 2 seconds...\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78626/bib13434192_19080524_11631_140_0011_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 4 seconds...\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78626/bib13434192_19080524_11631_140_0011_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 8 seconds...\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78626/bib13434192_19080524_11631_140_0011_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 16 seconds...\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78626/bib13434192_19080524_11631_140_0011_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 32 seconds...\n",
      "Failed to process query 'La Croix salong': No data to export. The list of data frames is empty.\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78550/bib13434192_19080131_11631_29_0010_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 2 seconds...\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78550/bib13434192_19080131_11631_29_0010_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 4 seconds...\n",
      "Failed to fetch XML content from https://data.kb.se/dark-78550/bib13434192_19080131_11631_29_0010_alto.xml. Status code: 429\n",
      "Rate limited. Retrying in 8 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m output_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_newspaper_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_date\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewspaper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_of_newspaper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moldtimey_touringbot_prompt_for_deployment.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_filepath\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData processing and export completed successfully.\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed query \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/KBDownloader.py:232\u001b[0m, in \u001b[0;36mfetch_newspaper_data\u001b[0;34m(query, from_date, to_date, newspaper, prompt_filepath, output_filepath)\u001b[0m\n\u001b[1;32m    230\u001b[0m xml_urls \u001b[38;5;241m=\u001b[39m extract_xml_urls(api_response, page_ids)\n\u001b[1;32m    231\u001b[0m xml_content_by_page \u001b[38;5;241m=\u001b[39m fetch_xml_content(xml_urls)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_number, xml_content \u001b[38;5;129;01min\u001b[39;00m xml_content_by_page\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    233\u001b[0m     xml_string \u001b[38;5;241m=\u001b[39m xml_content\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    234\u001b[0m     page \u001b[38;5;241m=\u001b[39m Page(xml_content\u001b[38;5;241m=\u001b[39mxml_string)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/KBDownloader.py:108\u001b[0m, in \u001b[0;36mfetch_xml_content\u001b[0;34m(xml_urls, max_retries, initial_delay)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limited. Retrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[0;32m--> 108\u001b[0m     delay \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# For other status codes, break the retry loop\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main loop over the specified year range\n",
    "for year in range(start_year, start_year + years_to_crawl):\n",
    "    for half in range(2):  # Loop for each half of the year\n",
    "        if half == 0:\n",
    "            from_date = datetime(year, 1, 1)  # Start of the year\n",
    "            to_date = datetime(year, 6, 30)  # End of June\n",
    "        else:\n",
    "            from_date = datetime(year, 7, 1)  # Start of July\n",
    "            to_date = datetime(year, 12, 31)  # End of the year\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            query = row['Lokal']\n",
    "            safe_query = \"\".join([c if c.isalnum() else \"_\" for c in query])\n",
    "            output_dir = f'extracted_data_{safe_query}_{today_date}'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_filepath = os.path.join(output_dir, f'extracted_data_{safe_query}_{today_date}')\n",
    "\n",
    "            result = fetch_newspaper_data(\n",
    "                query=query,\n",
    "                from_date=from_date.strftime('%Y-%m-%d'),  \n",
    "                to_date=to_date.strftime('%Y-%m-%d'),   \n",
    "\n",
    "                newspaper=name_of_newspaper,\n",
    "                prompt_filepath='oldtimey_touringbot_prompt_for_deployment.txt',\n",
    "                output_filepath=output_filepath\n",
    "            )\n",
    "            \n",
    "            if result.get('success') and result.get('message') == 'Data processing and export completed successfully.':\n",
    "                print(f\"Processed query '{query}' successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to process query '{query}': {result.get('message')}\")\n",
    "\n",
    "        # Wait for a minute after each half-year data collection\n",
    "        print(f\"waiting so KB does not get mad. Currently at{from_date} to {to_date}\")\n",
    "        time.sleep(60)\n",
    "\n",
    "print(\"All queries processed for all specified years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Clean up: Data created in step 1 is concatenated into jsonl file, folders and XLSX files deleted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the path where the JSONL files are stored and where to save the final concatenated JSONL file\n",
    "final_jsonl_filename = f'final_data_{today_date}_{name_of_newspaper}.jsonl'\n",
    "\n",
    "# Define the base directory to start the search and the path for the output JSONL file\n",
    "base_directory = '/Users/brandonfarnsworth/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/extracted_data29.05.'\n",
    "final_jsonl_filename = f'final_data_{today_date}_{name_of_newspaper}.jsonl'\n",
    "final_jsonl_filepath = os.path.join('/Users/brandonfarnsworth/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/', final_jsonl_filename)  # Adjust the output file path as needed\n",
    "\n",
    "print(f\"Looking for JSONL files in {base_directory}\")\n",
    "print(f\"Final concatenated file will be saved as {final_jsonl_filepath}\")\n",
    "\n",
    "# Open the output file once and write to it as we find JSONL files\n",
    "with open(final_jsonl_filepath, 'a') as f_out:\n",
    "    # Walk through the directory structure\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        print(f\"Checking directory: {root}\")\n",
    "        # Filter and process only JSONL files\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.jsonl'):  # This makes the check case-insensitive\n",
    "                full_path = os.path.join(root, file)\n",
    "                print(f\"Found JSONL file: {full_path}\")\n",
    "                with open(full_path, 'r') as f_in:\n",
    "                    f_out.write(f_in.read())\n",
    "                print(f\"Added contents of {file} to {final_jsonl_filepath}\")\n",
    "            else:\n",
    "                print(f\"Ignored file: {file}\")\n",
    "\n",
    "print(\"All JSONL files have been successfully concatenated.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
