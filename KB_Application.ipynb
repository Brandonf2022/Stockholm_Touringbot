{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file path: /Users/brandonfarnsworth/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Published Texts/Method Article Collecting Trace Data using LLM/Lokal_Datablad_Original.xlsx\n",
      "Today's date: 2024-06-30\n",
      "Start year for data fetching: 1908\n",
      "Years to crawl: 1\n",
      "Name of newspaper: Svenska Dagbladet\n",
      "Valid options for newspapers: ['Dagens nyheter', 'Svenska Dagbladet', 'Aftonbladet', 'Dagligt Allehanda']\n",
      "KB API Max Retries: 10\n",
      "KB API Max Retry Time: 300\n",
      "\n",
      "DataFrame preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lokal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Croix salong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Norra paviljongen i Trädgårdsföreningens lokal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wallmans lokal (Mäster Samuels gränd 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kungliga operan/Kungliga teatern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F.d. Kirsteinska huset (vid Clara) [Hotel W6 r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Lokal\n",
       "0                                    La Croix salong\n",
       "1     Norra paviljongen i Trädgårdsföreningens lokal\n",
       "2           Wallmans lokal (Mäster Samuels gränd 11)\n",
       "3                   Kungliga operan/Kungliga teatern\n",
       "4  F.d. Kirsteinska huset (vid Clara) [Hotel W6 r..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "import requests\n",
    "from KBDownloader import fetch_newspaper_data\n",
    "import os\n",
    "\n",
    "# Load configuration\n",
    "def load_config(config_path='config.yaml'):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "# Load the Excel file\n",
    "venue_list = pd.read_excel(config['venue_list'])\n",
    "\n",
    "# Get today's date\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Setup backoff for KB requests\n",
    "@backoff.on_exception(backoff.expo, \n",
    "                      (requests.exceptions.RequestException, requests.exceptions.HTTPError),\n",
    "                      max_tries=config['max_retries'],\n",
    "                      max_time=config['max_retry_time'])\n",
    "def fetch_with_backoff(*args, **kwargs):\n",
    "    return fetch_newspaper_data(*args, **kwargs)\n",
    "\n",
    "\n",
    "# Initialize start date for data fetching\n",
    "start_year = config['start_year']\n",
    "years_to_crawl = config['years_to_crawl']\n",
    "name_of_newspaper = config['newspaper']\n",
    "\n",
    "# Print out variables and assumptions\n",
    "print(f\"Excel file path: {config['venue_list']}\")\n",
    "print(f\"Today's date: {today_date}\")\n",
    "print(f\"Start year for data fetching: {config['start_year']}\")\n",
    "print(f\"Years to crawl: {config['years_to_crawl']}\")\n",
    "print(f\"Name of newspaper: {config['newspaper']}\")\n",
    "print(f\"Valid options for newspapers: {config['valid_newspapers']}\")\n",
    "print(f\"KB API Max Retries: {config['max_retries']}\")\n",
    "print(f\"KB API Max Retry Time: {config['max_retry_time']}\")\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify loading\n",
    "print(\"\\nDataFrame preview:\")\n",
    "display(venue_list.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Iteratively go through the venues (Lokale) and return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed query 'La Croix salong' successfully.\n",
      "Processed query 'Norra paviljongen i Trädgårdsföreningens lokal' successfully.\n",
      "Failed to process query 'Wallmans lokal (Mäster Samuels gränd 11)': No data to export. The list of data frames is empty.\n",
      "Processed query 'Kungliga operan/Kungliga teatern' successfully.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://data.kb.se/search?to=1908-06-30&from=1908-01-01&isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2F2ldhmx8d4mcrlq9%23it&q=F.d.+Kirsteinska+huset+%28vid+Clara%29+%5BHotel+W6+resp.+Hotel+Continental%5D&searchGranularity=part",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m output_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_newspaper_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_date\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewspaper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_of_newspaper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moldtimey_touringbot_prompt_for_deployment.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_filepath\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData processing and export completed successfully.\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed query \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/KBDownloader.py:220\u001b[0m, in \u001b[0;36mfetch_newspaper_data\u001b[0;34m(query, from_date, to_date, newspaper, prompt_filepath, output_filepath)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m collection_id:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid newspaper name provided\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 220\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_swedish_newspapers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown error\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/KBDownloader.py:52\u001b[0m, in \u001b[0;36msearch_swedish_newspapers\u001b[0;34m(to_date, from_date, collection_id, query)\u001b[0m\n\u001b[1;32m     49\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     51\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(base_url, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This will raise an HTTPError for bad responses\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://data.kb.se/search?to=1908-06-30&from=1908-01-01&isPartOf.%40id=https%3A%2F%2Flibris.kb.se%2F2ldhmx8d4mcrlq9%23it&q=F.d.+Kirsteinska+huset+%28vid+Clara%29+%5BHotel+W6+resp.+Hotel+Continental%5D&searchGranularity=part"
     ]
    }
   ],
   "source": [
    "# Main loop over the specified year range\n",
    "for year in range(start_year, start_year + years_to_crawl):\n",
    "    for half in range(2):  # Loop for each half of the year\n",
    "        if half == 0:\n",
    "            from_date = datetime(year, 1, 1)  # Start of the year\n",
    "            to_date = datetime(year, 6, 30)  # End of June\n",
    "        else:\n",
    "            from_date = datetime(year, 7, 1)  # Start of July\n",
    "            to_date = datetime(year, 12, 31)  # End of the year\n",
    "        \n",
    "        for index, row in venue_list.iterrows():\n",
    "            query = row['Lokal']\n",
    "            safe_query = \"\".join([c if c.isalnum() else \"_\" for c in query])\n",
    "            output_dir = f'extracted_data_{safe_query}_{today_date}'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_filepath = os.path.join(output_dir, f'extracted_data_{safe_query}_{today_date}')\n",
    "            \n",
    "            try:\n",
    "                result = fetch_newspaper_data(\n",
    "                    query=query,\n",
    "                    from_date=from_date.strftime('%Y-%m-%d'),\n",
    "                    to_date=to_date.strftime('%Y-%m-%d'),\n",
    "                    newspaper=name_of_newspaper,\n",
    "                    prompt_filepath='oldtimey_touringbot_prompt_for_deployment.txt',\n",
    "                    output_filepath=output_filepath\n",
    "                )\n",
    "                \n",
    "                if result.get('success'):\n",
    "                    print(f\"Processed query '{query}' successfully.\")\n",
    "                else:\n",
    "                    print(f\"Failed to process query '{query}': {result.get('message')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query '{query}': {str(e)}\")\n",
    "                print(\"Continuing with the next query...\")\n",
    "            \n",
    "            # Wait for a minute after each query\n",
    "            print(f\"Waiting so KB does not get mad. Currently at {from_date} to {to_date}\")\n",
    "            time.sleep(60)\n",
    "\n",
    "print(\"All queries processed for all specified years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Clean up: Data created in step 1 is concatenated into jsonl file, folders and XLSX files deleted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the path where the JSONL files are stored and where to save the final concatenated JSONL file\n",
    "final_jsonl_filename = f'final_data_{today_date}_{name_of_newspaper}.jsonl'\n",
    "\n",
    "# Define the base directory to start the search and the path for the output JSONL file\n",
    "base_directory = '/Users/brandonfarnsworth/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/extracted_data29.05.'\n",
    "final_jsonl_filename = f'final_data_{today_date}_{name_of_newspaper}.jsonl'\n",
    "final_jsonl_filepath = os.path.join('/Users/brandonfarnsworth/Library/Mobile Documents/com~apple~CloudDocs/Post-Phd/Quantitative Work/Software/Oldtimey_touringbot/', final_jsonl_filename)  # Adjust the output file path as needed\n",
    "\n",
    "print(f\"Looking for JSONL files in {base_directory}\")\n",
    "print(f\"Final concatenated file will be saved as {final_jsonl_filepath}\")\n",
    "\n",
    "# Open the output file once and write to it as we find JSONL files\n",
    "with open(final_jsonl_filepath, 'a') as f_out:\n",
    "    # Walk through the directory structure\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        print(f\"Checking directory: {root}\")\n",
    "        # Filter and process only JSONL files\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.jsonl'):  # This makes the check case-insensitive\n",
    "                full_path = os.path.join(root, file)\n",
    "                print(f\"Found JSONL file: {full_path}\")\n",
    "                with open(full_path, 'r') as f_in:\n",
    "                    f_out.write(f_in.read())\n",
    "                print(f\"Added contents of {file} to {final_jsonl_filepath}\")\n",
    "            else:\n",
    "                print(f\"Ignored file: {file}\")\n",
    "\n",
    "print(\"All JSONL files have been successfully concatenated.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
