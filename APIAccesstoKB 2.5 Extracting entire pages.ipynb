{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed successfully. Data is ready for export.\n",
      "Data exported to Excel file at extracted_data.xlsx\n",
      "Data exported to JSON Lines file at extracted_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def search_swedish_newspapers(to_date, from_date, collection_id, query):\n",
    "    base_url = 'https://data.kb.se/search'\n",
    "    params = {\n",
    "        'to': to_date,\n",
    "        'from': from_date,\n",
    "        'isPartOf.@id': collection_id,\n",
    "        'q': query,\n",
    "        'searchGranularity': 'part'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except ValueError:\n",
    "            return {'error': 'Invalid JSON response'}\n",
    "    else:\n",
    "        return {'error': response.status_code, 'message': response.text}\n",
    "\n",
    "# Adjust date ranges and collection IDs as needed\n",
    "from_date = '1908-07-01'\n",
    "to_date = '1908-09-30'\n",
    "collection_id = 'https://libris.kb.se/2ldhmx8d4mcrlq9#it'  # Svenska Dagbladet\n",
    "query = 'konsert'\n",
    "\n",
    "def extract_urls(result):\n",
    "    base_url = 'https://data.kb.se'\n",
    "    details = []\n",
    "\n",
    "    for hit in result['hits']:\n",
    "        part_number = hit.get('part')\n",
    "        page_number = hit.get('page')\n",
    "        page_id = hit.get('@id')  # Capture the exact page ID\n",
    "        package_id = hit.get('hasFilePackage', {}).get('@id', '').split('/')[-1]\n",
    "\n",
    "        if part_number and page_number and package_id and page_id:\n",
    "            url = f\"{base_url}/{package_id}/part/{part_number}/page/{page_number}\"\n",
    "            details.append({\n",
    "                'part_number': part_number,\n",
    "                'page_number': page_number,\n",
    "                'package_id': package_id,\n",
    "                'url': url,\n",
    "                'page_id': page_id  # Include the page ID in the details\n",
    "            })\n",
    "    \n",
    "    return details\n",
    "\n",
    "def extract_xml_urls(api_response, page_ids):\n",
    "    xml_urls = {}\n",
    "    parts_list = api_response.get('hasPart', [])  # Ensure this is the correct key to navigate the nested structure\n",
    "\n",
    "    for part in parts_list:\n",
    "        pages = part.get('hasPartList', [])\n",
    "        for page in pages:\n",
    "            if page['@id'] in page_ids:\n",
    "                includes = page.get('includes', [])\n",
    "                for include in includes:\n",
    "                    if 'alto.xml' in include['@id']:\n",
    "                        page_number = int(page['@id'].split('/')[-1].replace('page', ''))\n",
    "                        xml_urls[page_number] = include['@id']\n",
    "\n",
    "    return xml_urls\n",
    "\n",
    "def fetch_xml_content(xml_urls):\n",
    "    xml_content_by_page = {}\n",
    "    for page_number, url in xml_urls.items():\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            xml_content_by_page[page_number] = response.content\n",
    "        else:\n",
    "            print(f\"Failed to fetch XML content from {url}. Status code: {response.status_code}\")\n",
    "    return xml_content_by_page\n",
    "\n",
    "class Page:\n",
    "    def __init__(self, xml_path=None, xml_content=None) -> None:\n",
    "        if xml_path is not None:\n",
    "            self.load_xml_path(xml_path)\n",
    "        elif xml_content is not None:\n",
    "            self.load_xml(xml_content)\n",
    "        else:\n",
    "            raise ValueError(\"No xml path or content provided.\")\n",
    "\n",
    "    def load_xml_path(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            xml = f.read()\n",
    "        self.load_xml(xml)\n",
    "\n",
    "    def load_xml(self, xml):\n",
    "        soup = bs(xml, features=\"xml\")\n",
    "        self.soup = soup\n",
    "\n",
    "    def extract_date(self):\n",
    "        file_name_tag = self.soup.find(\"fileName\")\n",
    "        if file_name_tag:\n",
    "            file_name = file_name_tag.get_text()\n",
    "            date_match = re.search(r'_(\\d{8})_', file_name)\n",
    "            if date_match:\n",
    "                date_str = date_match.group(1)\n",
    "                formatted_date = f\"{date_str[0:4]}.{date_str[4:6]}.{date_str[6:8]}\"\n",
    "                return formatted_date\n",
    "        return None\n",
    "\n",
    "    def composed_blocks_from_keyword(self, keyword):\n",
    "        composed_blocks = []\n",
    "        token = self.soup.find(\"String\", attrs={\"CONTENT\": re.compile(re.escape(keyword), re.IGNORECASE)})\n",
    "        while token:\n",
    "            block = self.token_to_composed_block(token)\n",
    "            if block:\n",
    "                composed_blocks.append(block)\n",
    "            token = token.find_next(\"String\", attrs={\"CONTENT\": re.compile(re.escape(keyword), re.IGNORECASE)})\n",
    "        return composed_blocks\n",
    "\n",
    "    def token_to_composed_block(self, token):\n",
    "        composed_block = token.find_parent(\"ComposedBlock\")\n",
    "        if composed_block:\n",
    "            text_lines = composed_block.find_all(\"TextLine\")\n",
    "            content = \"\\n\".join(\n",
    "                \" \".join(string[\"CONTENT\"] for string in text_line.find_all(\"String\"))\n",
    "                for text_line in text_lines\n",
    "            )\n",
    "            return content\n",
    "        return None\n",
    "def read_system_message():\n",
    "    try:\n",
    "        with open('oldtimey_touringbot_prompt_for_deployment.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return \"You are a helpful assistant.\"\n",
    "\n",
    "def row_to_json(row):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    \n",
    "    system_message_content = read_system_message()\n",
    "    system_message = {\"role\": \"system\", \"content\": system_message_content}\n",
    "    \n",
    "    user_content_parts = [str(row[col]) for col in row.index if col not in ['System Content', 'Package ID', 'Part', 'Page']]\n",
    "    user_message = {\"role\": \"user\", \"content\": \" \".join(user_content_parts)}\n",
    "    \n",
    "    custom_id = f\"{row['Package ID']}-{row['Part']}-{row['Page']}-{counter}\"\n",
    "    \n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-3.5-turbo-0125\",\n",
    "            \"messages\": [system_message, user_message],\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    result = search_swedish_newspapers(to_date, from_date, collection_id, query)\n",
    "    if 'error' in result:\n",
    "        print(f\"Error fetching data: {result['error']}\")\n",
    "        return\n",
    "\n",
    "    detailed_info = extract_urls(result)\n",
    "    all_data_frames = []\n",
    "    page_ids = [info['page_id'] for info in detailed_info]  # Collect page IDs\n",
    "\n",
    "    for info in detailed_info:\n",
    "        url = info['url']\n",
    "        response = requests.get(url, headers={'Accept': 'application/json'})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            api_response = response.json()\n",
    "            xml_urls = extract_xml_urls(api_response, page_ids)\n",
    "            xml_content_by_page = fetch_xml_content(xml_urls)\n",
    "\n",
    "            for page_number, xml_content in xml_content_by_page.items():\n",
    "                xml_string = xml_content.decode('utf-8')\n",
    "                page = Page(xml_content=xml_string)\n",
    "\n",
    "                date = page.extract_date()\n",
    "                matching_composed_blocks = page.composed_blocks_from_keyword(query)\n",
    "\n",
    "                if matching_composed_blocks:\n",
    "                    df = pd.DataFrame(matching_composed_blocks, columns=[\"ComposedBlock Content\"])\n",
    "                    df['Date'] = date\n",
    "                    df['Package ID'] = info['package_id']\n",
    "                    df['Part'] = info['part_number']\n",
    "                    df['Page'] = page_number\n",
    "\n",
    "                    all_data_frames.append(df)\n",
    "                else:\n",
    "                    print(f\"No matching composed blocks found for page number {page_number}.\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "\n",
    "        # Remove duplicates in 'ComposedBlock Content' column\n",
    "        final_df = final_df.drop_duplicates(subset=[\"ComposedBlock Content\"])\n",
    "\n",
    "        if not final_df.empty:\n",
    "            print(\"Data processing completed successfully. Data is ready for export.\")\n",
    "\n",
    "            # Export to Excel\n",
    "            excel_output_path = \"extracted_data.xlsx\"\n",
    "            final_df.to_excel(excel_output_path, index=False)\n",
    "            print(f\"Data exported to Excel file at {excel_output_path}\")\n",
    "\n",
    "            # Export to JSON Lines using row_to_json function\n",
    "            jsonl_output_path = \"extracted_data.jsonl\"\n",
    "            with open(jsonl_output_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "                for _, row in final_df.iterrows():\n",
    "                    json_row = row_to_json(row)\n",
    "                    jsonl_file.write(json.dumps(json_row) + '\\n')\n",
    "            print(f\"Data exported to JSON Lines file at {jsonl_output_path}\")\n",
    "\n",
    "        else:\n",
    "            print(\"No data to export after aggregation.\")\n",
    "    else:\n",
    "        print(\"No data to export. The list of data frames is empty.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
